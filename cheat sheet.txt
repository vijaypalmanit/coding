df.rename(columns = {‘temperature’: ‘temp’, ‘event’:’eventtype’}) -- how to rename columns in a dataframe

df[df.event.isin([‘Rain’,’Sunny’])] -- how to filter Data by multiple categories

df.select_dtypes(include=['float64', 'int64']) -- how to select dataframe with specific datatypes

df.select_dtypes(include=[np.number])

df.select_dtypes(exclude=['object'])

data.salary.cat.reorder_categories(['low','medium','high'])   -- how to reorder categories

level_map = {1: 'high', 2: 'medium', 3: 'low'}
df['c_level'] = df['c'].map(level_map) 

df[df['ID'].isin(['A001','C022',...])]

data.salary = data.salary.cat.codes -- convert categories into numbers

data['churn'].value_counts(normalize=True)*100 -- percentage of each category

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42)  -- how to split train test data

data.dropna(axis='columns',how='all',inplace=True)  -- How to drop the columns with all NA values 


---------- How to find the best parameter for model (Hyperparameter tuning)-----
depth = [i for i in range(5,21,1)]

samples = [i for i in range(50,500,50)]

parameters = dict(max_depth=depth, min_samples_leaf=samples)

from sklearn.model_selection import GridSearchCV

param_search = GridSearchCV(model, parameters)

param_search.fit(features_train, target_train)

print(param_search.best_params_)

-------------------------------------------------------------------------------

data.groupby('month')[['duration']].sum()    -- how to get dataframe output after pandas aggrgation

grouped = df.groupby('customer')

grouped.groups  -- how to list groups in groupby object

grouped.get_group('Aaron Hendrickson') -- how to find a specific group in groupby object

grouped.size() - find the size of each group

df.groupby(pd.Grouper(key='Date',freq='Y')).size()  -- Group by year

OUT:
Date
2014-12-31    19956
2015-12-31    20054
2016-12-31    20133
2017-12-31    20079
2018-12-31    19778


df.groupby(df['Sales Rep'].str.split(' ').str[0]).size() --Group by the first name of sales rep

df.groupby(df['Sales Rep'].apply(lambda x: 'William' in x)).size() --Grouping by whether or not there is a “William” in the name of the rep

df.groupby('state').agg({'crime':[sum,mean,count],
						 'district':'first',
						 'date':lambda x: max(x)-1})  -- how to apply multiple functions to columns
						 
						 

df.groupby('state').agg(total_no_of_crimes=('crime','sum'),
						 first_district_in_data=('district','first'),
						 day_before_last_day=('date',lambda x: max(x)-1))    -- how to rename aggregated columns
						 
						 
						 
df.groupby([1, 1]).agg(foo=('A', lambda x: x.max()), bar=('A', lambda x: x.min())) -- error is raised when same source column is used with multiple lambdas



df['filled_weight'] = df.groupby('gender')['weight'].transform(lambda grp: grp.fillna(np.mean(grp)) --How to Fill missing values with the group’s mean
)



df.groupby([df.index.year, df.index.quarter])["co"].agg(["max", "min"]).rename_axis(["year", "quarter"]) -- how to rename aggragated columns

df.groupby("outlet", sort=False)["title"].apply(lambda ser: ser.str.contains("Fed").sum()).nlargest(10)  -- how to list top ten results


-------------------------------------------------------------------------------
pandas datatypes
----------------
object : This data type is used for strings (i.e., sequences of characters)
int64 
float64 
bool 
datetime64
timedelta : Used to represent the difference between datetimes
category




pd.to_numeric(df['column'],errors='ignore') -- how to ignore errors while converting datatypes
pd.to_numeric(df['column'],errors='coerce') -- how to turn the offending values into np.nan values while converting datatypes

invoices['Date of Meal'] = pd.to_datetime(invoices['Date of Meal'],format='%Y%d%m') -- how to convert to datetime



-------------------------------------------------------------------------------
There are three different accessors:
.dt
.str
.cat

startswith(<substring>), endswith(<substring>), contains(<substring>) checks for the presence of a substring


---------------preparing for machine learning interview questions in python------

df.dropna(axis='rows', thresh=3) --the thresh parameter lets you specify a minimum number of non-null values for the row/column to be kept:

dff.fillna(dff.mean()) 
          A         B         C
0  0.271860 -0.424972  0.567020
1  0.276232 -1.087401 -0.673690
2  0.113648 -1.478427  0.524988
3 -0.140857  0.577046 -1.715002
4 -0.140857 -0.401419 -1.157892
5 -1.344312 -0.401419 -0.293543
6 -0.109050  1.643563 -0.293543
7  0.357021 -0.674600 -0.293543
8 -0.968914 -1.294524  0.413738
9  0.276662 -0.472035 -0.013960


In [57]: df
Out[57]: 
   one       two     three
a  NaN -0.282863 -1.509059
c  NaN  1.212112 -0.173215
e  NaN  0.000000  0.000000
f  NaN  0.000000  0.000000
h  NaN -0.706771 -1.039575

In [58]: df.dropna(axis=0)
Out[58]: 
Empty DataFrame
Columns: [one, two, three]
Index: []

In [59]: df.dropna(axis=1)
Out[59]: 
        two     three
a -0.282863 -1.509059
c  1.212112 -0.173215
e  0.000000  0.000000
f  0.000000  0.000000
h -0.706771 -1.039575


# for each column, get value counts in decreasing order and take the index (value) of most common class
>colors
      X1     X2
0    NaN  Green
1    Red  Green
2   Blue    Red
3    Red   Blue
4    NaN  Green
5    Red   Blue
6  Green    NaN
7    NaN    Red
8   Blue  Green
9    Red    NaN
colors = colors.apply(lambda x: x.fillna(x.value_counts().index[0])) ---impute with Most Common Class in each column

>colors

      X1     X2
0    Red  Green
1    Red  Green
2   Blue    Red
3    Red   Blue
4    Red  Green
5    Red   Blue
6  Green  Green
7    Red    Red
8   Blue  Green
9    Red  Green


------ Use of simple imputer to impute missing values---------

from sklearn.impute import SimpleImputer

numeric_cols = loan_data.select_dtypes(include=[np.number])

imp_mean = SimpleImputer(strategy='mean') -- use strategy='most_frequent' to impute with mode

loans_imp_mean = imp_mean.fit_transform(numeric_cols)

loans_imp_meanDF = pd.DataFrame(loans_imp_mean, columns=numeric_cols.columns)

